# -*- coding: utf-8 -*-
"""FINAL.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1F059OJ7A9MgFr5wZJSbAwUX5a6E-VtGq
"""

import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt

from google.colab import drive
drive.mount('/content/drive')

train_data=pd.read_csv(r"/content/drive/MyDrive/train.csv")
test_data=pd.read_csv(r"/content/drive/MyDrive/test.csv")

train_data.head()

train_data = train_data.drop(['nom_5','nom_6','nom_7','nom_8','nom_9','ord_4', 'ord_5'], axis = 1)

train_data.head()

train_data.info()

train_data.describe()

train_data

train_data.target.value_counts()

train_data.columns

train_data.isna()

train_data.isnull().sum()

train_data.isnull().sum().sum()

train_data.dtypes

print("Categorical Variables:")
cv=train_data.select_dtypes(include=['object']).columns
cv

print("Numerical Variables:")
nv=train_data._get_numeric_data().columns
nv

print(train_data['target'].value_counts())

gf=sns.countplot(x=train_data['target'])
sum=len(train_data)
for p in gf.patches:
    print(sum)

print(train_data['bin_3'].unique())
print(train_data['bin_4'].unique())
print(train_data['nom_0'].unique())
print(train_data['nom_1'].unique())
print(train_data['nom_2'].unique())
print(train_data['nom_3'].unique())
print(train_data['nom_4'].unique())
print(train_data['ord_1'].unique())
print(train_data['ord_2'].unique())
print(train_data['ord_3'].unique())

train_data['bin_3'].replace(['T','F'],[0,1],inplace=True)
train_data['bin_4'].replace(['Y','N'],[0,1],inplace=True)
train_data['nom_0'].replace(['Green' ,'Blue' ,'Red'],[0,1,2],inplace=True)
train_data['nom_1'].replace(['Triangle' ,'Trapezoid', 'Polygon' ,'Square', 'Star', 'Circle'],[0,1,2,3,4,5],inplace=True)
train_data['nom_2'].replace(['Snake', 'Hamster', 'Lion' ,'Cat' ,'Dog' ,'Axolotl'],[0,1,2,3,4,5],inplace=True)
train_data['nom_3'].replace(['Finland', 'Russia', 'Canada' ,'Costa Rica' ,'China', 'India'],[0,1,2,3,4,5],inplace=True)
train_data['nom_4'].replace(['Bassoon' ,'Piano' ,'Theremin' ,'Oboe'],[0,1,2,3],inplace=True)
train_data['ord_1'].replace(['Grandmaster' ,'Expert' ,'Novice' ,'Contributor', 'Master'],[0,1,2,3,4],inplace=True)
train_data['ord_2'].replace(['Cold' ,'Hot', 'Lava Hot', 'Boiling Hot', 'Freezing', 'Warm'],[0,1,2,3,4,5],inplace=True)
train_data['ord_3'].replace(['h', 'a', 'i', 'j', 'g' ,'e' ,'d', 'b', 'k', 'f', 'l', 'n', 'o', 'c', 'm'],[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14],inplace=True)

train_data.head()

test_data = test_data.drop(['nom_5','nom_6','nom_7','nom_8','nom_9','ord_4', 'ord_5'], axis = 1)

test_data['bin_3'].replace(['T','F'],[0,1],inplace=True)
test_data['bin_4'].replace(['Y','N'],[0,1],inplace=True)
test_data['nom_0'].replace(['Green' ,'Blue' ,'Red'],[0,1,2],inplace=True)
test_data['nom_1'].replace(['Triangle' ,'Trapezoid', 'Polygon' ,'Square', 'Star', 'Circle'],[0,1,2,3,4,5],inplace=True)
test_data['nom_2'].replace(['Snake', 'Hamster', 'Lion' ,'Cat' ,'Dog' ,'Axolotl'],[0,1,2,3,4,5],inplace=True)
test_data['nom_3'].replace(['Finland', 'Russia', 'Canada' ,'Costa Rica' ,'China', 'India'],[0,1,2,3,4,5],inplace=True)
test_data['nom_4'].replace(['Bassoon' ,'Piano' ,'Theremin' ,'Oboe'],[0,1,2,3],inplace=True)
test_data['ord_1'].replace(['Grandmaster' ,'Expert' ,'Novice' ,'Contributor', 'Master'],[0,1,2,3,4],inplace=True)
test_data['ord_2'].replace(['Cold' ,'Hot', 'Lava Hot', 'Boiling Hot', 'Freezing', 'Warm'],[0,1,2,3,4,5],inplace=True)
test_data['ord_3'].replace(['h', 'a', 'i', 'j', 'g' ,'e' ,'d', 'b', 'k', 'f', 'l', 'n', 'o', 'c', 'm'],[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14],inplace=True)

test_data.head()

corr_matrix = train_data.corr()

# Create the heatmap using Seaborn
sns.set(style='white')
plt.figure(figsize=(25,10))
sns.heatmap(corr_matrix, annot=True, cmap='coolwarm')

# Show the plot
plt.show()

X = train_data.drop(['id'], axis = 1)

print(X.head())

X.head()

trained_model = X
trained_model.to_csv('train_preprocess.csv',index = False)

from sklearn.model_selection import train_test_split

y = train_data['target']
X = train_data.drop(columns=['target','id'], axis = 1)

# split into train and test dataset
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, stratify=y, random_state=42)
X_train.shape, X_test.shape, y_train.shape, y_test.shape

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.metrics import roc_curve, auc
from xgboost import XGBClassifier
from sklearn.svm import SVC
from sklearn.linear_model import LogisticRegression
from sklearn.naive_bayes import GaussianNB
from sklearn.ensemble import RandomForestClassifier

# Train XGBoost model
xgb_model = XGBClassifier(random_state=42)
xgb_model.fit(X_train, y_train)

#Train Logistic Regression model
lr_model = LogisticRegression(random_state=42,max_iter=10000)
lr_model.fit(X_train, y_train)

#Train Random Forest model
rf_model = RandomForestClassifier(n_estimators=100, random_state=42)
rf_model.fit(X_train, y_train)

# Make predictions for all models
#y_pred_xgb = xgb_model.predict_proba(X_test)[:, 1]
y_pred_lr = lr_model.predict_proba(X_test)[:, 1]
#y_pred_rf = rf_model.predict_proba(X_test)[:,1]

# Calculate ROC curves for both models
#fpr_xgb, tpr_xgb, _ = roc_curve(y_test, y_pred_xgb)
fpr_lr, tpr_lr, _ = roc_curve(y_test, y_pred_lr)
#fpr_rf, tpr_rf, _ = roc_curve(y_test, y_pred_rf)

# Calculate AUC for both models
#auc_xgb = auc(fpr_xgb, tpr_xgb)
auc_lr = auc(fpr_lr, tpr_lr)
#auc_rf = auc(fpr_rf, tpr_rf)

# Plot the ROC curves for all models in a single graph
#plt.plot(fpr_xgb, tpr_xgb, label=f'XGBoost (AUC = {auc_xgb:.2f})')
plt.plot(fpr_lr, tpr_lr, label=f'Logistic Regression (AUC = {auc_lr:.2f})')
#plt.plot(fpr_rf, tpr_rf, label=f'Random Forest (AUC = {auc_rf:.2f})')

# Add a legend and axis labels
plt.legend()
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('ROC Curves')
plt.show()

test_id=test_data['id']
X1 = test_data.drop(columns=['id'], axis = 1)
y_pred_xgb = xgb_model.predict_proba(X1)[:,1]
for i in y_pred_xgb[:10]:
    print(np.around(i,decimals=2))

y_pred_xgb = xgb_model.predict_proba(X1)[:,1]
for i in y_pred_xgb[:20]:
    print(np.around(i,decimals=2))

trained_model = X
trained_model.to_csv('train_preprocess.csv',index = False)

pip install gradio

import gradio as gr
from gradio.components import *

train_data.index.name="index"
index = [x for x in range(0,300000)]
train_data.index=index
train_data

import pickle
pickle.dump(xgb_model,open('csv.xgb','wb'))
model = pickle.load(open('csv.xgb','rb'))

def make_prediction( bin_0,bin_1,bin_2,bin_3,bin_4,nom_0,nom_1,nom_2,nom_3,nom_4,ord_0,ord_1,ord_2,ord_3,day,month):
    # Create input dataframe
    input_df = pd.DataFrame([[bin_0,bin_1,bin_2,bin_3,bin_4,nom_0,nom_1,nom_2,nom_3,nom_4,ord_0,ord_1,ord_2,ord_3,day,month]],columns=[ 'bin_0','bin_1','bin_2','bin_3','bin_4','nom_0','nom_1','nom_2','nom_3','nom_4','ord_0','ord_1','ord_2','ord_3','day','month'])
    lr_pred = lr_model.predict_proba(input_df)[:, 1][0]
    return {'Logistic Regression': lr_pred}
# Define input components for Gradio interface
bin_0 = gr.components.Number(label='bin_0')
bin_1 = gr.components.Number(label='bin_1')
bin_2 = gr.components.Number(label='bin_2')
bin_3 = gr.components.Number(label='bin_3')
bin_4 = gr.components.Number(label='bin_4')
nom_0 = gr.components.Number(label='nom_0')
nom_1 = gr.components.Number(label='nom_1')
nom_2 = gr.components.Number(label='nom_2')
nom_3 = gr.components.Number(label='nom_3')
nom_4 = gr.components.Number(label='nom_4')
ord_0 = gr.components.Number(label='ord_0')
ord_1 = gr.components.Number(label='ord_1')
ord_2 = gr.components.Number(label='ord_2')
ord_3 = gr.components.Number(label='ord_3')
day = gr.components.Number(label='day')
month = gr.components.Number(label='month')

css_code='.gradio-container {background-image:url("https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcQxgf6t6I6_N4W-8Clm0impndLdfMzZmkXV4k_vnfEZMg&usqp=CAU&ec=48665701");height: 100%;background-position: center;background-repeat: no-repeat;background-size:Â cover;}'
# Create Gradio interface
gr.Interface(fn=make_prediction, inputs=[bin_0,bin_1,bin_2,bin_3,bin_4,nom_0,nom_1,nom_2,nom_3,nom_4,ord_0,ord_1,ord_2,ord_3,day,month],
outputs='label', title='BINARY CLASSIFICATION', description='Predict the probability',css=css_code).launch()